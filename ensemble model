# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, f1_score
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# Step 1: Synthesize Data
np.random.seed(42)
n_samples = 1000  # Number of patients increased to 10,000
data = {
    'Age': np.random.randint(15, 80, n_samples),
    'Gender': np.random.choice(['Male', 'Female'], n_samples),
    'Cough_Duration': np.random.randint(0, 60, n_samples),  # days
    'Fever': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),  # 0: No, 1: Yes
    'Weight_Loss': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
    'HIV_Status': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),  # 0: Negative, 1: Positive
    'Smoking': np.random.choice([0, 1], n_samples, p=[0.75, 0.25]),
    'Malnutrition': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),
    'Chest_Xray': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # 0: Normal, 1: Abnormal
    'GeneXpert': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),  # 0: Negative, 1: Positive
    'TB_Status': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])  # Target variable
}

df = pd.DataFrame(data)

# Encode categorical features
df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})  # Encode Gender

# Separate features (X) and target (y)
X = df.drop(columns=['TB_Status'])
y = df['TB_Status']

# Step 2: Handle Imbalance using SMOTE
smote = SMOTE(random_state=5)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split resampled data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 3: Train Random Forest with class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

rf_model = RandomForestClassifier(n_estimators=200, max_depth=7, random_state=5, class_weight=class_weight_dict)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
rf_probs = rf_model.predict_proba(X_test)[:, 1]

print("\nRandom Forest Performance:")
print("Accuracy:", accuracy_score(y_test, rf_preds))
print("AUC:", roc_auc_score(y_test, rf_probs))
print("F1-Score:", f1_score(y_test, rf_preds))
print("Classification Report:\n", classification_report(y_test, rf_preds))

# Step 4: Train XGBoost with scale_pos_weight
scale_pos_weight = class_weights[0] / class_weights[1]

xgb_model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)
xgb_probs = xgb_model.predict_proba(X_test)[:, 1]

print("\nXGBoost Performance:")
print("Accuracy:", accuracy_score(y_test, xgb_preds))
print("AUC:", roc_auc_score(y_test, xgb_probs))
print("F1-Score:", f1_score(y_test, xgb_preds))
print("Classification Report:\n", classification_report(y_test, xgb_preds))

# Step 5: Create an Ensemble using Logistic Regression
ensemble_data = np.column_stack((rf_probs, xgb_probs))
ensemble_model = LogisticRegression()
ensemble_model.fit(ensemble_data, y_test)

ensemble_probs = ensemble_model.predict_proba(ensemble_data)[:, 1]
ensemble_preds = ensemble_model.predict(ensemble_data)

print("\nEnsemble Performance:")
print("Accuracy:", accuracy_score(y_test, ensemble_preds))
print("AUC:", roc_auc_score(y_test, ensemble_probs))
print("F1-Score:", f1_score(y_test, ensemble_preds))
print("Classification Report:\n", classification_report(y_test, ensemble_preds))

# Step 6: Visualize Feature Importance
plt.figure(figsize=(12, 6))

# Random Forest Feature Importance
plt.subplot(1, 2, 1)
plt.barh(X.columns, rf_model.feature_importances_)
plt.title("Random Forest Feature Importance")

# XGBoost Feature Importance
plt.subplot(1, 2, 2)
plt.barh(X.columns, xgb_model.feature_importances_)
plt.title("XGBoost Feature Importance")

plt.tight_layout()
plt.show()
